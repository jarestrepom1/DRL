{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fed25f7",
   "metadata": {},
   "source": [
    "# Assault (Atari) con DQN + PER\n",
    "\n",
    "Este notebook entrena un agente DQN con Prioritized Experience Replay (PER) en ALE/Assault-v5.\n",
    "Incluye preprocesamiento, checkpoints, logging con TensorBoard y evaluacion vs baseline aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca8b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: gymnasium 1.2.3 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
      "\u001b[0mAutoROM will download the Atari 2600 ROMs.\n",
      "They will be installed to:\n",
      "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
      "\n",
      "Existing ROMs will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias (compatible con Colab)\n",
    "!pip -q install gymnasium[atari,accept-rom-license] ale-py autorom torch tensorboard\n",
    "!AutoROM --accept-license\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import ale_py  # Registra el namespace ALE\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "ART_DIR = os.path.join(ROOT_DIR, \"artifacts\", \"assault\")\n",
    "CKPT_DIR = os.path.join(ART_DIR, \"checkpoints\")\n",
    "LOG_DIR = os.path.join(ART_DIR, \"logs\")\n",
    "VIDEO_DIR = os.path.join(ART_DIR, \"videos\")\n",
    "for d in [CKPT_DIR, LOG_DIR, VIDEO_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Dispositivo:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Gymnasium:\", gym.__version__)\n",
    "try:\n",
    "    print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
    "except Exception:\n",
    "    print(\"nvidia-smi no disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8660cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaciones locales (sin importar src/)\n",
    "import math\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium.wrappers import AtariPreprocessing, TransformReward\n",
    "\n",
    "\n",
    "class SimpleFrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, num_stack=4):\n",
    "        super().__init__(env)\n",
    "        self.num_stack = num_stack\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "        low = np.repeat(env.observation_space.low[None, ...], num_stack, axis=0)\n",
    "        high = np.repeat(env.observation_space.high[None, ...], num_stack, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_stack):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.stack(list(self.frames), axis=0)\n",
    "\n",
    "\n",
    "def make_atari_env(\n",
    "    game_id: str,\n",
    "    seed: int,\n",
    "    frame_skip: int = 4,\n",
    "    clip_rewards: bool = False,\n",
    "    render_mode: str | None = None,\n",
    "):\n",
    "    env = gym.make(\n",
    "        game_id,\n",
    "        frameskip=1,\n",
    "        repeat_action_probability=0.0,\n",
    "        render_mode=render_mode,\n",
    "    )\n",
    "    env = AtariPreprocessing(\n",
    "        env,\n",
    "        frame_skip=frame_skip,\n",
    "        grayscale_obs=True,\n",
    "        screen_size=84,\n",
    "        scale_obs=False,\n",
    "        terminal_on_life_loss=False,\n",
    "    )\n",
    "    if clip_rewards:\n",
    "        env = TransformReward(env, lambda r: max(-1.0, min(1.0, r)))\n",
    "    env = SimpleFrameStack(env, num_stack=4)\n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1, dtype=np.float32)\n",
    "        self.data_pointer = 0\n",
    "\n",
    "    @property\n",
    "    def total(self) -> float:\n",
    "        return float(self.tree[0])\n",
    "\n",
    "    def add(self, priority: float):\n",
    "        idx = self.data_pointer + self.capacity - 1\n",
    "        self.update(idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        return idx\n",
    "\n",
    "    def update(self, idx: int, priority: float) -> None:\n",
    "        change = priority - self.tree[idx]\n",
    "        self.tree[idx] = priority\n",
    "        parent = (idx - 1) // 2\n",
    "        while True:\n",
    "            self.tree[parent] += change\n",
    "            if parent == 0:\n",
    "                break\n",
    "            parent = (parent - 1) // 2\n",
    "\n",
    "    def get(self, s: float):\n",
    "        idx = 0\n",
    "        while True:\n",
    "            left = 2 * idx + 1\n",
    "            right = left + 1\n",
    "            if left >= len(self.tree):\n",
    "                leaf = idx\n",
    "                break\n",
    "            if s <= self.tree[left]:\n",
    "                idx = left\n",
    "            else:\n",
    "                s -= self.tree[left]\n",
    "                idx = right\n",
    "        data_idx = leaf - self.capacity + 1\n",
    "        return leaf, self.tree[leaf], data_idx\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity: int, alpha: float = 0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.data = [None] * capacity\n",
    "        self.max_priority = 1.0\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        idx = self.tree.add(self.max_priority)\n",
    "        self.data[idx - self.capacity + 1] = (obs, action, reward, next_obs, done)\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int, beta: float = 0.4):\n",
    "        if self.tree.total == 0:\n",
    "            raise ValueError(\"No hay prioridades en el buffer\")\n",
    "        indices = []\n",
    "        priorities = []\n",
    "        samples = []\n",
    "        segment = self.tree.total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            data = None\n",
    "            for _ in range(20):\n",
    "                s = np.random.uniform(segment * i, segment * (i + 1))\n",
    "                idx, p, data_idx = self.tree.get(s)\n",
    "                data = self.data[data_idx]\n",
    "                if data is not None:\n",
    "                    indices.append(idx)\n",
    "                    priorities.append(p)\n",
    "                    samples.append(data)\n",
    "                    break\n",
    "            if data is None:\n",
    "                valid = [j for j, d in enumerate(self.data) if d is not None]\n",
    "                data_idx = int(np.random.choice(valid))\n",
    "                idx = data_idx + self.capacity - 1\n",
    "                p = self.tree.tree[idx]\n",
    "                indices.append(idx)\n",
    "                priorities.append(p)\n",
    "                samples.append(self.data[data_idx])\n",
    "        probs = np.array(priorities, dtype=np.float32) / self.tree.total\n",
    "        weights = (self.size * probs) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        obs, actions, rewards, next_obs, dones = map(np.array, zip(*samples))\n",
    "        return obs, actions, rewards, next_obs, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, p in zip(indices, priorities):\n",
    "            priority = float(p) ** self.alpha\n",
    "            self.tree.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"capacity\": self.capacity,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"tree\": self.tree.tree.copy(),\n",
    "            \"data\": self.data,\n",
    "            \"max_priority\": self.max_priority,\n",
    "            \"size\": self.size,\n",
    "            \"data_pointer\": self.tree.data_pointer,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        self.capacity = state[\"capacity\"]\n",
    "        self.alpha = state[\"alpha\"]\n",
    "        self.tree = SumTree(self.capacity)\n",
    "        self.tree.tree = state[\"tree\"]\n",
    "        self.tree.data_pointer = state[\"data_pointer\"]\n",
    "        self.data = state[\"data\"]\n",
    "        self.max_priority = state[\"max_priority\"]\n",
    "        self.size = state[\"size\"]\n",
    "\n",
    "\n",
    "class EpsilonSchedule:\n",
    "    def __init__(self, start: float, end: float, decay_steps: int):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay_steps = decay_steps\n",
    "\n",
    "    def value(self, step: int) -> float:\n",
    "        frac = min(step / self.decay_steps, 1.0)\n",
    "        return self.start + frac * (self.end - self.start)\n",
    "\n",
    "\n",
    "class PERBetaSchedule:\n",
    "    def __init__(self, start: float, end: float, steps: int):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "\n",
    "    def value(self, step: int) -> float:\n",
    "        frac = min(step / self.steps, 1.0)\n",
    "        return self.start + frac * (self.end - self.start)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_actions: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class DQNPerAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_shape,\n",
    "        num_actions: int,\n",
    "        device: str,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-4,\n",
    "        target_update_interval: int = 10_000,\n",
    "        buffer_size: int = 200_000,\n",
    "        alpha: float = 0.6,\n",
    "        eps_schedule: EpsilonSchedule | None = None,\n",
    "        beta_schedule: PERBetaSchedule | None = None,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.step_count = 0\n",
    "\n",
    "        in_channels = obs_shape[0]\n",
    "        self.online = QNetwork(in_channels, num_actions).to(device)\n",
    "        self.target = QNetwork(in_channels, num_actions).to(device)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.online.parameters(), lr=lr)\n",
    "        self.replay = PrioritizedReplayBuffer(buffer_size, alpha=alpha)\n",
    "\n",
    "        self.eps_schedule = eps_schedule or EpsilonSchedule(1.0, 0.05, 1_000_000)\n",
    "        self.beta_schedule = beta_schedule or PERBetaSchedule(0.4, 1.0, 1_000_000)\n",
    "\n",
    "    def select_action(self, obs: np.ndarray) -> int:\n",
    "        eps = self.eps_schedule.value(self.step_count)\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.online(obs_t)\n",
    "        return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def update(self, batch_size: int):\n",
    "        if len(self.replay) < batch_size:\n",
    "            return None\n",
    "\n",
    "        beta = self.beta_schedule.value(self.step_count)\n",
    "        obs, actions, rewards, next_obs, dones, indices, weights = self.replay.sample(batch_size, beta)\n",
    "\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        next_obs_t = torch.tensor(next_obs, dtype=torch.float32, device=self.device)\n",
    "        actions_t = torch.tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards_t = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        dones_t = torch.tensor(dones, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        weights_t = torch.tensor(weights, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "\n",
    "        q_values = self.online(obs_t).gather(1, actions_t)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target(next_obs_t).max(dim=1, keepdim=True)[0]\n",
    "            target = rewards_t + self.gamma * (1.0 - dones_t) * next_q\n",
    "\n",
    "        td_error = target - q_values\n",
    "        loss = F.smooth_l1_loss(q_values, target, reduction=\"none\")\n",
    "        loss = (loss * weights_t).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        priorities = td_error.detach().abs().cpu().numpy().squeeze() + 1e-6\n",
    "        self.replay.update_priorities(indices, priorities)\n",
    "\n",
    "        if self.step_count % self.target_update_interval == 0:\n",
    "            self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        return float(loss.item()), float(td_error.detach().abs().mean().item())\n",
    "\n",
    "    def save_state(self):\n",
    "        return {\n",
    "            \"online\": self.online.state_dict(),\n",
    "            \"target\": self.target.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"step_count\": self.step_count,\n",
    "            \"replay\": self.replay.state_dict(),\n",
    "            \"gamma\": self.gamma,\n",
    "            \"target_update_interval\": self.target_update_interval,\n",
    "        }\n",
    "\n",
    "    def load_state(self, state):\n",
    "        self.online.load_state_dict(state[\"online\"])\n",
    "        self.target.load_state_dict(state[\"target\"])\n",
    "        self.optimizer.load_state_dict(state[\"optimizer\"])\n",
    "        self.step_count = int(state[\"step_count\"])\n",
    "        self.replay.load_state_dict(state[\"replay\"])\n",
    "        self.gamma = float(state.get(\"gamma\", self.gamma))\n",
    "        self.target_update_interval = int(state.get(\"target_update_interval\", self.target_update_interval))\n",
    "\n",
    "\n",
    "def save_checkpoint(state: dict, checkpoint_dir: str, step: int) -> str:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    ckpt_path = os.path.join(checkpoint_dir, f\"checkpoint_{step}.pth\")\n",
    "    torch.save(state, ckpt_path)\n",
    "    return ckpt_path\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_path: str, device: str):\n",
    "    return torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "\n",
    "def save_config(config: dict, checkpoint_dir: str) -> str:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    config_path = os.path.join(checkpoint_dir, \"config.json\")\n",
    "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    return config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e249e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros y configuracion\n",
    "config = {\n",
    "    \"env_id\": \"ALE/Assault-v5\",\n",
    "    \"frame_skip\": 4,\n",
    "    \"clip_rewards\": True,\n",
    "    \"total_steps\": 2_000_000,\n",
    "    \"learning_starts\": 50_000,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer_size\": 200_000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"target_update_interval\": 10_000,\n",
    "    \"checkpoint_interval\": 200_000,\n",
    "    \"eps_start\": 1.0,\n",
    "    \"eps_end\": 0.05,\n",
    "    \"eps_decay_steps\": 1_000_000,\n",
    "    \"beta_start\": 0.4,\n",
    "    \"beta_end\": 1.0,\n",
    "    \"beta_steps\": 1_000_000,\n",
    "    \"alpha\": 0.6,\n",
    "    \"seed\": SEED\n",
    "}\n",
    "save_config(config, CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear entorno\n",
    "env = make_atari_env(\n",
    "    config[\"env_id\"],\n",
    "    seed=SEED,\n",
    "    frame_skip=config[\"frame_skip\"],\n",
    "    clip_rewards=config[\"clip_rewards\"],\n",
    ")\n",
    "num_actions = env.action_space.n\n",
    "obs_shape = env.observation_space.shape\n",
    "print(\"Obs shape:\", obs_shape, \"Acciones:\", num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5156fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agente\n",
    "eps_schedule = EpsilonSchedule(\n",
    "    config[\"eps_start\"],\n",
    "    config[\"eps_end\"],\n",
    "    config[\"eps_decay_steps\"],\n",
    ")\n",
    "beta_schedule = PERBetaSchedule(\n",
    "    config[\"beta_start\"],\n",
    "    config[\"beta_end\"],\n",
    "    config[\"beta_steps\"],\n",
    ")\n",
    "agent = DQNPerAgent(\n",
    "    obs_shape=obs_shape,\n",
    "    num_actions=num_actions,\n",
    "    device=DEVICE,\n",
    "    gamma=config[\"gamma\"],\n",
    "    lr=config[\"lr\"],\n",
    "    target_update_interval=config[\"target_update_interval\"],\n",
    "    buffer_size=config[\"buffer_size\"],\n",
    "    alpha=config[\"alpha\"],\n",
    "    eps_schedule=eps_schedule,\n",
    "    beta_schedule=beta_schedule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994567d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer de TensorBoard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "\n",
    "def obs_to_array(obs):\n",
    "    arr = np.asarray(obs)\n",
    "    # Si ya viene en formato (C, H, W), no transponer\n",
    "    if arr.ndim == 3 and arr.shape[0] in (1, 4):\n",
    "        return arr\n",
    "    # Si viene como (H, W, C), convertir a (C, H, W)\n",
    "    if arr.ndim == 3:\n",
    "        return np.transpose(arr, (2, 0, 1))\n",
    "    return arr\n",
    "\n",
    "\n",
    "def preprocess(obs):\n",
    "    arr = obs_to_array(obs)\n",
    "    return arr.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de entrenamiento con checkpoints\n",
    "resume_path = None  # define ruta de checkpoint para reanudar\n",
    "if resume_path:\n",
    "    state = load_checkpoint(resume_path, device=DEVICE)\n",
    "    agent.load_state(state)\n",
    "    print(\"Reanudado desde\", resume_path)\n",
    "\n",
    "obs, info = env.reset()\n",
    "episode_reward = 0.0\n",
    "episode_len = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(1, config[\"total_steps\"] + 1):\n",
    "    agent.step_count = step\n",
    "    obs_proc = preprocess(obs)\n",
    "    action = agent.select_action(obs_proc)\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    agent.replay.add(obs_to_array(obs), action, reward, obs_to_array(next_obs), float(done))\n",
    "    episode_reward += reward\n",
    "    episode_len += 1\n",
    "\n",
    "    if step > config[\"learning_starts\"]:\n",
    "        update_out = agent.update(config[\"batch_size\"])\n",
    "        if update_out is not None:\n",
    "            loss, td_error = update_out\n",
    "            writer.add_scalar(\"train/loss\", loss, step)\n",
    "            writer.add_scalar(\"train/td_error\", td_error, step)\n",
    "\n",
    "    eps_value = agent.eps_schedule.value(step)\n",
    "    writer.add_scalar(\"train/epsilon\", eps_value, step)\n",
    "\n",
    "    if done:\n",
    "        writer.add_scalar(\"rollout/episode_return\", episode_reward, step)\n",
    "        writer.add_scalar(\"rollout/episode_length\", episode_len, step)\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        episode_len = 0\n",
    "    else:\n",
    "        obs = next_obs\n",
    "\n",
    "    if step % config[\"checkpoint_interval\"] == 0:\n",
    "        state = agent.save_state()\n",
    "        ckpt_path = save_checkpoint(state, CKPT_DIR, step)\n",
    "        print(\"Guardado\", ckpt_path)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Tiempo de entrenamiento (s): {train_time:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers de evaluacion\n",
    "def greedy_action(agent, obs):\n",
    "    obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q = agent.online(obs_t)\n",
    "    return int(torch.argmax(q, dim=1).item())\n",
    "\n",
    "def run_eval(env_id, n_episodes=10, seed_offset=0, use_random=False, record_video=False):\n",
    "    eval_env = make_atari_env(\n",
    "        env_id,\n",
    "        seed=SEED + seed_offset,\n",
    "        frame_skip=config[\"frame_skip\"],\n",
    "        clip_rewards=False,\n",
    "        render_mode=\"rgb_array\" if record_video else None,\n",
    "    )\n",
    "    if record_video:\n",
    "        eval_env = RecordVideo(eval_env, video_folder=VIDEO_DIR, name_prefix=\"assault_eval\")\n",
    "    rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = eval_env.reset(seed=SEED + seed_offset + ep)\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            obs_proc = preprocess(obs)\n",
    "            if use_random:\n",
    "                action = eval_env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(agent, obs_proc)\n",
    "            obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            ep_reward += reward\n",
    "        rewards.append(ep_reward)\n",
    "    eval_env.close()\n",
    "    mean_r = float(np.mean(rewards))\n",
    "    std_r = float(np.std(rewards))\n",
    "    return rewards, mean_r, std_r\n",
    "\n",
    "agent.online.eval()\n",
    "eval_rewards, eval_mean, eval_std = run_eval(config[\"env_id\"], n_episodes=10)\n",
    "print(f\"DQN+PER recompensa media: {eval_mean:.2f} +/- {eval_std:.2f}\")\n",
    "\n",
    "rand_rewards, rand_mean, rand_std = run_eval(config[\"env_id\"], n_episodes=10, use_random=True)\n",
    "print(f\"Politica aleatoria recompensa media: {rand_mean:.2f} +/- {rand_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc80b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar un video corto de evaluacion\n",
    "_ = run_eval(config[\"env_id\"], n_episodes=1, seed_offset=9999, record_video=True)\n",
    "print(\"Video guardado en:\", VIDEO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f87ce",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "Ejecuta en Colab:\n",
    "\n",
    "```\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir artifacts/assault/logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8aa1bf",
   "metadata": {},
   "source": [
    "## Reporte tecnico (completar despues del entrenamiento)\n",
    "- Algoritmo: DQN + PER\n",
    "- Hiperparametros: ver config.json\n",
    "- Librerias y versiones: impresas en la celda de setup\n",
    "- Hardware: salida de nvidia-smi en la celda de setup\n",
    "- Tiempo de entrenamiento: impreso al finalizar\n",
    "- Resultados: media/desviacion en 10 episodios + baseline aleatorio\n",
    "- Conclusiones: agrega observaciones y siguientes pasos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
