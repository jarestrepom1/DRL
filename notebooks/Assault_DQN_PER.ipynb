{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fed25f7",
   "metadata": {},
   "source": [
    "# Assault (Atari) con DQN + PER\n",
    "\n",
    "Este notebook entrena un agente DQN con Prioritized Experience Replay (PER) en ALE/Assault-v5.\n",
    "Incluye preprocesamiento, checkpoints, logging con TensorBoard y evaluacion vs baseline aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca8b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias (compatible con Colab)\n",
    "!pip -q install gymnasium[atari,accept-rom-license] ale-py autorom torch tensorboard\n",
    "!AutoROM --accept-license\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from src.envs.atari import make_atari_env\n",
    "from src.agents.dqn_per import DQNPerAgent, EpsilonSchedule, PERBetaSchedule\n",
    "from src.utils.checkpoints import save_checkpoint, load_checkpoint, save_config\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "ART_DIR = os.path.join(ROOT_DIR, \"artifacts\", \"assault\")\n",
    "CKPT_DIR = os.path.join(ART_DIR, \"checkpoints\")\n",
    "LOG_DIR = os.path.join(ART_DIR, \"logs\")\n",
    "VIDEO_DIR = os.path.join(ART_DIR, \"videos\")\n",
    "for d in [CKPT_DIR, LOG_DIR, VIDEO_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Dispositivo:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Gymnasium:\", gym.__version__)\n",
    "try:\n",
    "    print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
    "except Exception:\n",
    "    print(\"nvidia-smi no disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e249e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros y configuracion\n",
    "config = {\n",
    "    \"env_id\": \"ALE/Assault-v5\",\n",
    "    \"frame_skip\": 4,\n",
    "    \"clip_rewards\": True,\n",
    "    \"total_steps\": 2_000_000,\n",
    "    \"learning_starts\": 50_000,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer_size\": 200_000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"target_update_interval\": 10_000,\n",
    "    \"checkpoint_interval\": 200_000,\n",
    "    \"eps_start\": 1.0,\n",
    "    \"eps_end\": 0.05,\n",
    "    \"eps_decay_steps\": 1_000_000,\n",
    "    \"beta_start\": 0.4,\n",
    "    \"beta_end\": 1.0,\n",
    "    \"beta_steps\": 1_000_000,\n",
    "    \"alpha\": 0.6,\n",
    "    \"seed\": SEED\n",
    "}\n",
    "save_config(config, CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear entorno\n",
    "env = make_atari_env(\n",
    "    config[\"env_id\"],\n",
    "    seed=SEED,\n",
    "    frame_skip=config[\"frame_skip\"],\n",
    "    clip_rewards=config[\"clip_rewards\"],\n",
    ")\n",
    "num_actions = env.action_space.n\n",
    "obs_shape = env.observation_space.shape\n",
    "print(\"Obs shape:\", obs_shape, \"Acciones:\", num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5156fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agente\n",
    "eps_schedule = EpsilonSchedule(\n",
    "    config[\"eps_start\"],\n",
    "    config[\"eps_end\"],\n",
    "    config[\"eps_decay_steps\"],\n",
    ")\n",
    "beta_schedule = PERBetaSchedule(\n",
    "    config[\"beta_start\"],\n",
    "    config[\"beta_end\"],\n",
    "    config[\"beta_steps\"],\n",
    ")\n",
    "agent = DQNPerAgent(\n",
    "    obs_shape=obs_shape,\n",
    "    num_actions=num_actions,\n",
    "    device=DEVICE,\n",
    "    gamma=config[\"gamma\"],\n",
    "    lr=config[\"lr\"],\n",
    "    target_update_interval=config[\"target_update_interval\"],\n",
    "    buffer_size=config[\"buffer_size\"],\n",
    "    alpha=config[\"alpha\"],\n",
    "    eps_schedule=eps_schedule,\n",
    "    beta_schedule=beta_schedule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994567d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer de TensorBoard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "def obs_to_array(obs):\n",
    "    arr = np.asarray(obs)\n",
    "    if arr.ndim == 3:\n",
    "        arr = np.transpose(arr, (2, 0, 1))\n",
    "    return arr\n",
    "\n",
    "def preprocess(obs):\n",
    "    arr = obs_to_array(obs)\n",
    "    return arr.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de entrenamiento con checkpoints\n",
    "resume_path = None  # define ruta de checkpoint para reanudar\n",
    "if resume_path:\n",
    "    state = load_checkpoint(resume_path, device=DEVICE)\n",
    "    agent.load_state(state)\n",
    "    print(\"Reanudado desde\", resume_path)\n",
    "\n",
    "obs, info = env.reset()\n",
    "episode_reward = 0.0\n",
    "episode_len = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(1, config[\"total_steps\"] + 1):\n",
    "    agent.step_count = step\n",
    "    obs_proc = preprocess(obs)\n",
    "    action = agent.select_action(obs_proc)\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    agent.replay.add(obs_to_array(obs), action, reward, obs_to_array(next_obs), float(done))\n",
    "    episode_reward += reward\n",
    "    episode_len += 1\n",
    "\n",
    "    if step > config[\"learning_starts\"]:\n",
    "        update_out = agent.update(config[\"batch_size\"])\n",
    "        if update_out is not None:\n",
    "            loss, td_error = update_out\n",
    "            writer.add_scalar(\"train/loss\", loss, step)\n",
    "            writer.add_scalar(\"train/td_error\", td_error, step)\n",
    "\n",
    "    eps_value = agent.eps_schedule.value(step)\n",
    "    writer.add_scalar(\"train/epsilon\", eps_value, step)\n",
    "\n",
    "    if done:\n",
    "        writer.add_scalar(\"rollout/episode_return\", episode_reward, step)\n",
    "        writer.add_scalar(\"rollout/episode_length\", episode_len, step)\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        episode_len = 0\n",
    "    else:\n",
    "        obs = next_obs\n",
    "\n",
    "    if step % config[\"checkpoint_interval\"] == 0:\n",
    "        state = agent.save_state()\n",
    "        ckpt_path = save_checkpoint(state, CKPT_DIR, step)\n",
    "        print(\"Guardado\", ckpt_path)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Tiempo de entrenamiento (s): {train_time:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers de evaluacion\n",
    "def greedy_action(agent, obs):\n",
    "    obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q = agent.online(obs_t)\n",
    "    return int(torch.argmax(q, dim=1).item())\n",
    "\n",
    "def run_eval(env_id, n_episodes=10, seed_offset=0, use_random=False, record_video=False):\n",
    "    eval_env = make_atari_env(\n",
    "        env_id,\n",
    "        seed=SEED + seed_offset,\n",
    "        frame_skip=config[\"frame_skip\"],\n",
    "        clip_rewards=False,\n",
    "        render_mode=\"rgb_array\" if record_video else None,\n",
    "    )\n",
    "    if record_video:\n",
    "        eval_env = RecordVideo(eval_env, video_folder=VIDEO_DIR, name_prefix=\"assault_eval\")\n",
    "    rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = eval_env.reset(seed=SEED + seed_offset + ep)\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            obs_proc = preprocess(obs)\n",
    "            if use_random:\n",
    "                action = eval_env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(agent, obs_proc)\n",
    "            obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            ep_reward += reward\n",
    "        rewards.append(ep_reward)\n",
    "    eval_env.close()\n",
    "    mean_r = float(np.mean(rewards))\n",
    "    std_r = float(np.std(rewards))\n",
    "    return rewards, mean_r, std_r\n",
    "\n",
    "agent.online.eval()\n",
    "eval_rewards, eval_mean, eval_std = run_eval(config[\"env_id\"], n_episodes=10)\n",
    "print(f\"DQN+PER recompensa media: {eval_mean:.2f} +/- {eval_std:.2f}\")\n",
    "\n",
    "rand_rewards, rand_mean, rand_std = run_eval(config[\"env_id\"], n_episodes=10, use_random=True)\n",
    "print(f\"Politica aleatoria recompensa media: {rand_mean:.2f} +/- {rand_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc80b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar un video corto de evaluacion\n",
    "_ = run_eval(config[\"env_id\"], n_episodes=1, seed_offset=9999, record_video=True)\n",
    "print(\"Video guardado en:\", VIDEO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f87ce",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "Ejecuta en Colab:\n",
    "\n",
    "```\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir artifacts/assault/logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8aa1bf",
   "metadata": {},
   "source": [
    "## Reporte tecnico (completar despues del entrenamiento)\n",
    "- Algoritmo: DQN + PER\n",
    "- Hiperparametros: ver config.json\n",
    "- Librerias y versiones: impresas en la celda de setup\n",
    "- Hardware: salida de nvidia-smi en la celda de setup\n",
    "- Tiempo de entrenamiento: impreso al finalizar\n",
    "- Resultados: media/desviacion en 10 episodios + baseline aleatorio\n",
    "- Conclusiones: agrega observaciones y siguientes pasos"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
