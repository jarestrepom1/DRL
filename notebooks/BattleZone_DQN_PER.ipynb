{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BattleZone (Atari) con DQN + PER\n",
    "\n",
    "Este notebook entrena un agente DQN con Prioritized Experience Replay (PER) en `ALE/BattleZone-v5`.\n",
    "BattleZone es un juego de tanques en primera persona con perspectiva 3D, 18 acciones posibles y recompensas dispersas por destruccion de vehiculos enemigos.\n",
    "Incluye preprocesamiento Atari, checkpoints para reanudar entrenamiento, logging con TensorBoard, evaluacion formal vs baseline aleatorio, graficas y video final.\n",
    "\n",
    "## Setup Colab\n",
    "Ejecuta las siguientes celdas en orden. El setup instala dependencias, valida versiones, confirma GPU y realiza un smoke test del entorno."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setup optimizado para Colab con GPU\n",
    "import os\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Mejor debugging CUDA\n",
    "\n",
    "# Desinstalar TensorFlow para evitar conflictos\n",
    "!pip uninstall -y tensorflow tensorflow-gpu -q 2>/dev/null || true\n",
    "\n",
    "# Instalar dependencias core\n",
    "!pip -q install gymnasium[atari,accept-rom-license] ale-py autorom torch tensorboard\n",
    "\n",
    "# Descargar ROMs\n",
    "!AutoROM --accept-license -q\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# Forzar limpieza de memoria GPU\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "ROOT_DIR = \"/content\" if os.path.exists(\"/content\") else os.getcwd()\n",
    "ART_ROOT = os.path.join(ROOT_DIR, \"artifacts\", \"battlezone\")\n",
    "CKPT_DIR = os.path.join(ART_ROOT, \"checkpoints\")\n",
    "FINAL_DIR = os.path.join(ART_ROOT, \"final_model\")\n",
    "LOG_DIR = os.path.join(ART_ROOT, \"logs\")\n",
    "VIDEO_DIR = os.path.join(ART_ROOT, \"videos\")\n",
    "CONFIG_PATH = os.path.join(ART_ROOT, \"config.json\")\n",
    "for d in [CKPT_DIR, FINAL_DIR, LOG_DIR, VIDEO_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"=\"*80)\n",
    "print(\"SETUP FINALIZADO\")\n",
    "print(\"=\"*80)\n",
    "print(\"Dispositivo:\", DEVICE)\n",
    "print(\"Python:\", sys.version.replace(\"\\n\", \" \"))\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Gymnasium:\", gym.__version__)\n",
    "print(\"ale_py:\", ale_py.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"\\nnvidia-smi:\")\n",
    "    try:\n",
    "        print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
    "    except Exception as e:\n",
    "        print(\"nvidia-smi no disponible:\", e)\n",
    "else:\n",
    "    print(\"GPU: No disponible\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Smoke test: crear entorno, reset y step + GPU warmup\n",
    "print(\"Iniciando smoke test...\")\n",
    "try:\n",
    "    test_env = gym.make(\"ALE/BattleZone-v5\", frameskip=1, repeat_action_probability=0.0)\n",
    "    obs, info = test_env.reset(seed=SEED)\n",
    "    action = test_env.action_space.sample()\n",
    "    next_obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "    print(\"\\u2713 Smoke test OK\")\n",
    "    print(\"  obs.shape:\", obs.shape, \"| n_actions:\", test_env.action_space.n)\n",
    "    test_env.close()\n",
    "\n",
    "    # GPU warmup: torch tensor en CUDA\n",
    "    print(\"  GPU warmup...\", end=\" \")\n",
    "    warmup = torch.randn(1, 4, 84, 84, device=DEVICE)\n",
    "    _ = warmup * 2\n",
    "    torch.cuda.synchronize() if DEVICE == \"cuda\" else None\n",
    "    print(\"\\u2713\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Smoke test failed. Revisa instalaciones, ROMs, GPU y el runtime.\"\n",
    "    ) from e\n",
    "\n",
    "print(\"Todo listo para entrenar.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Monitoreo de RAM disponible\n",
    "import psutil\n",
    "ram = psutil.virtual_memory()\n",
    "print(\"=\"*80)\n",
    "print(\"MEMORIA DEL SISTEMA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"RAM total: {ram.total / (1024**3):.1f} GB\")\n",
    "print(f\"RAM disponible: {ram.available / (1024**3):.1f} GB\")\n",
    "print(f\"RAM usada: {ram.percent:.1f}%\")\n",
    "if ram.available / (1024**3) < 10:\n",
    "    print(\"\\n\\u26a0\\ufe0f  ADVERTENCIA: Menos de 10GB disponibles. Considera:\")\n",
    "    print(\"   - Reiniciar runtime\")\n",
    "    print(\"   - Reducir buffer_size a 50_000\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entorno, wrappers y PER\n",
    "Se definen wrappers consistentes para entrenamiento y evaluacion, junto con el buffer PER y la red Q.\n",
    "BattleZone usa las mismas transformaciones que otros entornos Atari: escala de grises, redimensionado a 84x84 y frame stacking de 4 frames."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Implementaciones locales\n",
    "import math\n",
    "import glob\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium.wrappers import AtariPreprocessing, TransformReward\n",
    "\n",
    "\n",
    "class SimpleFrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, num_stack=4):\n",
    "        super().__init__(env)\n",
    "        self.num_stack = num_stack\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "        low = np.repeat(env.observation_space.low[None, ...], num_stack, axis=0)\n",
    "        high = np.repeat(env.observation_space.high[None, ...], num_stack, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.num_stack):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.stack(list(self.frames), axis=0)\n",
    "\n",
    "\n",
    "def make_atari_env(\n",
    "    game_id: str,\n",
    "    seed: int,\n",
    "    frame_skip: int = 4,\n",
    "    clip_rewards: bool = False,\n",
    "    render_mode: str | None = None,\n",
    " ):\n",
    "    env = gym.make(\n",
    "        game_id,\n",
    "        frameskip=1,\n",
    "        repeat_action_probability=0.0,\n",
    "        render_mode=render_mode,\n",
    "    )\n",
    "    env = AtariPreprocessing(\n",
    "        env,\n",
    "        frame_skip=frame_skip,\n",
    "        grayscale_obs=True,\n",
    "        screen_size=84,\n",
    "        scale_obs=False,\n",
    "        terminal_on_life_loss=False,\n",
    "    )\n",
    "    if clip_rewards:\n",
    "        env = TransformReward(env, lambda r: max(-1.0, min(1.0, r)))\n",
    "    env = SimpleFrameStack(env, num_stack=4)\n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1, dtype=np.float32)\n",
    "        self.data_pointer = 0\n",
    "\n",
    "    @property\n",
    "    def total(self) -> float:\n",
    "        return float(self.tree[0])\n",
    "\n",
    "    def add(self, priority: float):\n",
    "        idx = self.data_pointer + self.capacity - 1\n",
    "        self.update(idx, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        return idx\n",
    "\n",
    "    def update(self, idx: int, priority: float) -> None:\n",
    "        change = priority - self.tree[idx]\n",
    "        self.tree[idx] = priority\n",
    "        parent = (idx - 1) // 2\n",
    "        while True:\n",
    "            self.tree[parent] += change\n",
    "            if parent == 0:\n",
    "                break\n",
    "            parent = (parent - 1) // 2\n",
    "\n",
    "    def get(self, s: float):\n",
    "        idx = 0\n",
    "        while True:\n",
    "            left = 2 * idx + 1\n",
    "            right = left + 1\n",
    "            if left >= len(self.tree):\n",
    "                leaf = idx\n",
    "                break\n",
    "            if s <= self.tree[left]:\n",
    "                idx = left\n",
    "            else:\n",
    "                s -= self.tree[left]\n",
    "                idx = right\n",
    "        data_idx = leaf - self.capacity + 1\n",
    "        return leaf, self.tree[leaf], data_idx\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity: int, alpha: float = 0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.data = [None] * capacity\n",
    "        self.max_priority = 1.0\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        idx = self.tree.add(self.max_priority)\n",
    "        if isinstance(obs, np.ndarray) and obs.dtype == np.uint8:\n",
    "            obs_u8 = obs\n",
    "        else:\n",
    "            obs_u8 = (np.clip(obs, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "        if isinstance(next_obs, np.ndarray) and next_obs.dtype == np.uint8:\n",
    "            next_obs_u8 = next_obs\n",
    "        else:\n",
    "            next_obs_u8 = (np.clip(next_obs, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "        self.data[idx - self.capacity + 1] = (obs_u8, action, reward, next_obs_u8, done)\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int, beta: float = 0.4):\n",
    "        if self.tree.total == 0:\n",
    "            raise ValueError(\"No hay prioridades en el buffer\")\n",
    "        indices = []\n",
    "        priorities = []\n",
    "        samples = []\n",
    "        segment = self.tree.total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            data = None\n",
    "            for _ in range(20):\n",
    "                s = np.random.uniform(segment * i, segment * (i + 1))\n",
    "                idx, p, data_idx = self.tree.get(s)\n",
    "                data = self.data[data_idx]\n",
    "                if data is not None:\n",
    "                    indices.append(idx)\n",
    "                    priorities.append(p)\n",
    "                    samples.append(data)\n",
    "                    break\n",
    "            if data is None:\n",
    "                valid = [j for j, d in enumerate(self.data) if d is not None]\n",
    "                data_idx = int(np.random.choice(valid))\n",
    "                idx = data_idx + self.capacity - 1\n",
    "                p = self.tree.tree[idx]\n",
    "                indices.append(idx)\n",
    "                priorities.append(p)\n",
    "                samples.append(self.data[data_idx])\n",
    "        probs = np.array(priorities, dtype=np.float32) / self.tree.total\n",
    "        weights = (self.size * probs) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        obs, actions, rewards, next_obs, dones = map(np.array, zip(*samples))\n",
    "        obs = obs.astype(np.float32) / 255.0\n",
    "        next_obs = next_obs.astype(np.float32) / 255.0\n",
    "        return obs, actions, rewards, next_obs, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, p in zip(indices, priorities):\n",
    "            priority = float(p) ** self.alpha\n",
    "            self.tree.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"capacity\": self.capacity,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"tree\": self.tree.tree.copy(),\n",
    "            \"data\": self.data,\n",
    "            \"max_priority\": self.max_priority,\n",
    "            \"size\": self.size,\n",
    "            \"data_pointer\": self.tree.data_pointer,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        self.capacity = state[\"capacity\"]\n",
    "        self.alpha = state[\"alpha\"]\n",
    "        self.tree = SumTree(self.capacity)\n",
    "        self.tree.tree = state[\"tree\"]\n",
    "        self.tree.data_pointer = state[\"data_pointer\"]\n",
    "        self.data = state[\"data\"]\n",
    "        self.max_priority = state[\"max_priority\"]\n",
    "        self.size = state[\"size\"]\n",
    "\n",
    "\n",
    "class EpsilonSchedule:\n",
    "    def __init__(self, start: float, end: float, decay_steps: int):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay_steps = decay_steps\n",
    "\n",
    "    def value(self, step: int) -> float:\n",
    "        frac = min(step / self.decay_steps, 1.0)\n",
    "        return self.start + frac * (self.end - self.start)\n",
    "\n",
    "\n",
    "class PERBetaSchedule:\n",
    "    def __init__(self, start: float, end: float, steps: int):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.steps = steps\n",
    "\n",
    "    def value(self, step: int) -> float:\n",
    "        frac = min(step / self.steps, 1.0)\n",
    "        return self.start + frac * (self.end - self.start)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_actions: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class DQNPerAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_shape,\n",
    "        num_actions: int,\n",
    "        device: str,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-4,\n",
    "        target_update_interval: int = 10_000,\n",
    "        buffer_size: int = 200_000,\n",
    "        alpha: float = 0.6,\n",
    "        eps_schedule: EpsilonSchedule | None = None,\n",
    "        beta_schedule: PERBetaSchedule | None = None,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.step_count = 0\n",
    "\n",
    "        in_channels = obs_shape[0]\n",
    "        self.online = QNetwork(in_channels, num_actions).to(device)\n",
    "        self.target = QNetwork(in_channels, num_actions).to(device)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.online.parameters(), lr=lr)\n",
    "        self.replay = PrioritizedReplayBuffer(buffer_size, alpha=alpha)\n",
    "\n",
    "        self.eps_schedule = eps_schedule or EpsilonSchedule(1.0, 0.05, 1_000_000)\n",
    "        self.beta_schedule = beta_schedule or PERBetaSchedule(0.4, 1.0, 1_000_000)\n",
    "\n",
    "    def select_action(self, obs: np.ndarray) -> int:\n",
    "        eps = self.eps_schedule.value(self.step_count)\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.online(obs_t)\n",
    "        return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def update(self, batch_size: int):\n",
    "        if len(self.replay) < batch_size:\n",
    "            return None\n",
    "\n",
    "        beta = self.beta_schedule.value(self.step_count)\n",
    "        obs, actions, rewards, next_obs, dones, indices, weights = self.replay.sample(batch_size, beta)\n",
    "\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        next_obs_t = torch.tensor(next_obs, dtype=torch.float32, device=self.device)\n",
    "        actions_t = torch.tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards_t = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        dones_t = torch.tensor(dones, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        weights_t = torch.tensor(weights, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "\n",
    "        q_values_all = self.online(obs_t)\n",
    "        q_values = q_values_all.gather(1, actions_t)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target(next_obs_t).max(dim=1, keepdim=True)[0]\n",
    "            target = rewards_t + self.gamma * (1.0 - dones_t) * next_q\n",
    "\n",
    "        td_error = target - q_values\n",
    "        loss = F.smooth_l1_loss(q_values, target, reduction=\"none\")\n",
    "        loss = (loss * weights_t).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        priorities = td_error.detach().abs().cpu().numpy().squeeze() + 1e-6\n",
    "        self.replay.update_priorities(indices, priorities)\n",
    "\n",
    "        if self.step_count % self.target_update_interval == 0:\n",
    "            self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        mean_q = float(q_values_all.mean().item())\n",
    "        return float(loss.item()), float(td_error.detach().abs().mean().item()), mean_q, float(beta)\n",
    "\n",
    "    def save_state(self):\n",
    "        return {\n",
    "            \"online\": self.online.state_dict(),\n",
    "            \"target\": self.target.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"step_count\": self.step_count,\n",
    "            \"replay\": self.replay.state_dict(),\n",
    "            \"gamma\": self.gamma,\n",
    "            \"target_update_interval\": self.target_update_interval,\n",
    "        }\n",
    "\n",
    "    def load_state(self, state):\n",
    "        self.online.load_state_dict(state[\"online\"])\n",
    "        self.target.load_state_dict(state[\"target\"])\n",
    "        self.optimizer.load_state_dict(state[\"optimizer\"])\n",
    "        self.step_count = int(state[\"step_count\"])\n",
    "        self.replay.load_state_dict(state[\"replay\"])\n",
    "        self.gamma = float(state.get(\"gamma\", self.gamma))\n",
    "        self.target_update_interval = int(state.get(\"target_update_interval\", self.target_update_interval))\n",
    "\n",
    "\n",
    "def is_checkpoint_valid(ckpt_path: str) -> bool:\n",
    "    \"\"\"Valida que el checkpoint no este corrupto o vacio\"\"\"\n",
    "    try:\n",
    "        file_size = os.path.getsize(ckpt_path)\n",
    "        if file_size < 1000:  # Checkpoint valido debe ser > 1KB\n",
    "            return False\n",
    "        # Intentar cargar para detectar corrupcion\n",
    "        torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  \\u2717 Checkpoint invalido: {ckpt_path} ({type(e).__name__})\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_checkpoint(state: dict, checkpoint_dir: str, step: int) -> str:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    ckpt_path = os.path.join(checkpoint_dir, f\"checkpoint_{step}.pth\")\n",
    "    torch.save(state, ckpt_path)\n",
    "    # Validar que se guardo correctamente\n",
    "    if not is_checkpoint_valid(ckpt_path):\n",
    "        os.remove(ckpt_path)\n",
    "        raise RuntimeError(f\"Checkpoint corrupto al guardar: {ckpt_path}\")\n",
    "    return ckpt_path\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_path: str, device: str):\n",
    "    \"\"\"Carga checkpoint con manejo de errores\"\"\"\n",
    "    try:\n",
    "        return torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando {ckpt_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_config(config: dict, output_path: str) -> str:\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def save_final_artifacts(agent: DQNPerAgent, output_dir: str, config: dict, config_path: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    torch.save(agent.online.state_dict(), os.path.join(output_dir, \"final_model.pth\"))\n",
    "    torch.save(agent.target.state_dict(), os.path.join(output_dir, \"final_target.pth\"))\n",
    "    torch.save(agent.optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pth\"))\n",
    "    save_config(config, config_path)\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir: str) -> str | None:\n",
    "    \"\"\"Obtiene el ultimo checkpoint valido\"\"\"\n",
    "    pattern = os.path.join(checkpoint_dir, \"checkpoint_*.pth\")\n",
    "    ckpts = glob.glob(pattern)\n",
    "    if not ckpts:\n",
    "        return None\n",
    "\n",
    "    def _step(p):\n",
    "        name = os.path.basename(p)\n",
    "        step_str = name.replace(\"checkpoint_\", \"\").replace(\".pth\", \"\")\n",
    "        return int(step_str)\n",
    "\n",
    "    ckpts.sort(key=_step, reverse=True)\n",
    "\n",
    "    # Buscar el primer checkpoint valido (de mas reciente a mas antiguo)\n",
    "    for ckpt in ckpts:\n",
    "        if is_checkpoint_valid(ckpt):\n",
    "            return ckpt\n",
    "        else:\n",
    "            # Limpiar checkpoints corruptos\n",
    "            try:\n",
    "                os.remove(ckpt)\n",
    "                print(f\"  Eliminado checkpoint corrupto: {ckpt}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Hiperparametros ajustados para BattleZone (18 acciones, recompensas dispersas)\n",
    "config = {\n",
    "    \"env_id\": \"ALE/BattleZone-v5\",\n",
    "    \"frame_skip\": 4,\n",
    "    \"clip_rewards\": True,\n",
    "    \"total_steps\": 3_000_000,       # Mas pasos: 18 acciones requieren mas exploracion\n",
    "    \"learning_starts\": 80_000,      # Mas warmup para cubrir espacio de acciones amplio\n",
    "    \"update_every\": 4,\n",
    "    \"batch_size\": 32,\n",
    "    \"buffer_size\": 100_000,          # RAM limitada en Colab (~3.5GB)\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 6.25e-5,                   # LR conservador (Rainbow paper) para 18 acciones\n",
    "    \"target_update_interval\": 10_000,\n",
    "    \"checkpoint_interval\": 500_000,\n",
    "    \"eval_interval\": 0,              # Solo eval final para ahorrar RAM\n",
    "    \"eval_episodes\": 3,\n",
    "    \"eps_start\": 1.0,\n",
    "    \"eps_end\": 0.01,\n",
    "    \"eps_decay_steps\": 1_500_000,    # Decay mas lento (50% del total, como en Assault)\n",
    "    \"beta_start\": 0.4,\n",
    "    \"beta_end\": 1.0,\n",
    "    \"beta_steps\": 2_000_000,         # Beta=1.0 al 67% del entrenamiento\n",
    "    \"alpha\": 0.6,\n",
    "    \"seed\": SEED\n",
    "}\n",
    "save_config(config, CONFIG_PATH)\n",
    "print(f\"Config guardada:\")\n",
    "print(f\"  env_id={config['env_id']}\")\n",
    "print(f\"  total_steps={config['total_steps']:,}\")\n",
    "print(f\"  buffer_size={config['buffer_size']:,} (~3.5GB RAM)\")\n",
    "print(f\"  lr={config['lr']}\")\n",
    "print(f\"  eps_decay_steps={config['eps_decay_steps']:,}\")\n",
    "print(f\"  eval_interval={config['eval_interval']} (deshabilitado para ahorrar RAM)\")\n",
    "print(f\"  checkpoint_interval={config['checkpoint_interval']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Diagnostico de checkpoints\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTICO DE CHECKPOINTS\")\n",
    "print(\"=\"*80)\n",
    "ckpts = glob.glob(os.path.join(CKPT_DIR, \"checkpoint_*.pth\"))\n",
    "if not ckpts:\n",
    "    print(\"\\u2713 Sin checkpoints previos (primera ejecucion)\")\n",
    "else:\n",
    "    print(f\"Encontrados {len(ckpts)} checkpoint(s):\")\n",
    "    def _step(p):\n",
    "        name = os.path.basename(p)\n",
    "        step_str = name.replace(\"checkpoint_\", \"\").replace(\".pth\", \"\")\n",
    "        return int(step_str)\n",
    "\n",
    "    ckpts.sort(key=_step)\n",
    "    for ckpt in ckpts:\n",
    "        size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
    "        valid = \"\\u2713\" if is_checkpoint_valid(ckpt) else \"\\u2717\"\n",
    "        print(f\"  {valid} {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion, entorno y agente\n",
    "Se inicializa el entorno BattleZone con wrappers Atari estandar y se construye el agente DQN+PER."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Crear entorno\n",
    "env = make_atari_env(\n",
    "    config[\"env_id\"],\n",
    "    seed=SEED,\n",
    "    frame_skip=config[\"frame_skip\"],\n",
    "    clip_rewards=config[\"clip_rewards\"],\n",
    ")\n",
    "num_actions = env.action_space.n\n",
    "obs_shape = env.observation_space.shape\n",
    "print(\"Obs shape:\", obs_shape, \"Acciones:\", num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Diagnostico: wrappers y observaciones\n",
    "def describe_env(env, label: str):\n",
    "    print(f\"[{label}] action_space={env.action_space}\")\n",
    "    print(f\"[{label}] obs_shape={env.observation_space.shape}, dtype={env.observation_space.dtype}\")\n",
    "    print(f\"[{label}] wrappers={env}\")\n",
    "\n",
    "describe_env(env, \"train\")\n",
    "\n",
    "# Verificar un entorno de evaluacion con los mismos wrappers\n",
    "eval_env_diag = make_atari_env(\n",
    "    config[\"env_id\"],\n",
    "    seed=SEED + 999,\n",
    "    frame_skip=config[\"frame_skip\"],\n",
    "    clip_rewards=config[\"clip_rewards\"],\n",
    "    render_mode=None,\n",
    ")\n",
    "describe_env(eval_env_diag, \"eval\")\n",
    "eval_env_diag.close()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Agente\n",
    "eps_schedule = EpsilonSchedule(\n",
    "    config[\"eps_start\"],\n",
    "    config[\"eps_end\"],\n",
    "    config[\"eps_decay_steps\"],\n",
    ")\n",
    "beta_schedule = PERBetaSchedule(\n",
    "    config[\"beta_start\"],\n",
    "    config[\"beta_end\"],\n",
    "    config[\"beta_steps\"],\n",
    ")\n",
    "agent = DQNPerAgent(\n",
    "    obs_shape=obs_shape,\n",
    "    num_actions=num_actions,\n",
    "    device=DEVICE,\n",
    "    gamma=config[\"gamma\"],\n",
    "    lr=config[\"lr\"],\n",
    "    target_update_interval=config[\"target_update_interval\"],\n",
    "    buffer_size=config[\"buffer_size\"],\n",
    "    alpha=config[\"alpha\"],\n",
    "    eps_schedule=eps_schedule,\n",
    "    beta_schedule=beta_schedule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Writer de TensorBoard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "\n",
    "def obs_to_array(obs):\n",
    "    arr = np.asarray(obs)\n",
    "    # Si ya viene en formato (C, H, W), no transponer\n",
    "    if arr.ndim == 3 and arr.shape[0] in (1, 4):\n",
    "        return arr\n",
    "    # Si viene como (H, W, C), convertir a (C, H, W)\n",
    "    if arr.ndim == 3:\n",
    "        return np.transpose(arr, (2, 0, 1))\n",
    "    return arr\n",
    "\n",
    "\n",
    "def preprocess(obs):\n",
    "    arr = obs_to_array(obs)\n",
    "    return arr.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y checkpoints\n",
    "Se ejecuta el loop principal con logging y guardado de checkpoints cada 500K pasos.\n",
    "El sistema de checkpoints permite reanudar el entrenamiento en caso de desconexion de Colab.\n",
    "Al final se exporta el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loop de entrenamiento optimizado para RAM limitada\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "print(\"Buscando checkpoints validos...\")\n",
    "resume_path = get_latest_checkpoint(CKPT_DIR)\n",
    "if resume_path:\n",
    "    try:\n",
    "        state = load_checkpoint(resume_path, device=DEVICE)\n",
    "        agent.load_state(state)\n",
    "        print(f\"\\u2713 Reanudado desde {resume_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\u2717 Error al reanudar: {e}. Iniciando entrenamiento desde cero.\")\n",
    "        resume_path = None\n",
    "else:\n",
    "    print(\"\\u2713 Sin checkpoints previos. Iniciando entrenamiento desde cero.\")\n",
    "\n",
    "\n",
    "def greedy_action(agent, obs):\n",
    "    obs_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q = agent.online(obs_t)\n",
    "    return int(torch.argmax(q, dim=1).item())\n",
    "\n",
    "\n",
    "def run_eval(env_id, n_episodes=10, seed_offset=0, use_random=False, record_video=False):\n",
    "    eval_env = make_atari_env(\n",
    "        env_id,\n",
    "        seed=SEED + seed_offset,\n",
    "        frame_skip=config[\"frame_skip\"],\n",
    "        clip_rewards=config[\"clip_rewards\"],\n",
    "        render_mode=\"rgb_array\" if record_video else None,\n",
    "    )\n",
    "    if record_video:\n",
    "        eval_env = RecordVideo(eval_env, video_folder=VIDEO_DIR, name_prefix=\"battlezone_eval\")\n",
    "    rewards = []\n",
    "    was_training = agent.online.training\n",
    "    agent.online.eval()\n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = eval_env.reset(seed=SEED + seed_offset + ep)\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            obs_proc = preprocess(obs)\n",
    "            if use_random:\n",
    "                action = eval_env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(agent, obs_proc)\n",
    "            obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            ep_reward += reward\n",
    "        rewards.append(ep_reward)\n",
    "    eval_env.close()\n",
    "    if was_training:\n",
    "        agent.online.train()\n",
    "    mean_r = float(np.mean(rewards))\n",
    "    std_r = float(np.std(rewards))\n",
    "    return rewards, mean_r, std_r\n",
    "\n",
    "\n",
    "obs, info = env.reset()\n",
    "episode_reward = 0.0\n",
    "episode_len = 0\n",
    "episode_returns = deque(maxlen=100)\n",
    "train_episode_returns = []\n",
    "train_episode_end_steps = []\n",
    "eval_history = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Limpieza agresiva cada N episodios\n",
    "episode_count = 0\n",
    "GC_EVERY_N_EPISODES = 50\n",
    "\n",
    "for step in range(1, config[\"total_steps\"] + 1):\n",
    "    agent.step_count = step\n",
    "    obs_proc = preprocess(obs)\n",
    "    action = agent.select_action(obs_proc)\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    next_obs_proc = preprocess(next_obs)\n",
    "    agent.replay.add(obs_proc, action, reward, next_obs_proc, float(done))\n",
    "    episode_reward += reward\n",
    "    episode_len += 1\n",
    "\n",
    "    if step > config[\"learning_starts\"] and step % config[\"update_every\"] == 0:\n",
    "        update_out = agent.update(config[\"batch_size\"])\n",
    "        if update_out is not None:\n",
    "            loss, td_error, mean_q, beta_val = update_out\n",
    "            # Solo loggear cada 1000 steps para reducir overhead\n",
    "            if step % 1000 == 0:\n",
    "                writer.add_scalar(\"train/loss\", loss, step)\n",
    "                writer.add_scalar(\"train/td_error\", td_error, step)\n",
    "                writer.add_scalar(\"train/mean_q\", mean_q, step)\n",
    "                writer.add_scalar(\"train/beta\", beta_val, step)\n",
    "\n",
    "    # Logging menos frecuente\n",
    "    if step % 5000 == 0:\n",
    "        eps_value = agent.eps_schedule.value(step)\n",
    "        beta_value = agent.beta_schedule.value(step)\n",
    "        writer.add_scalar(\"train/epsilon\", eps_value, step)\n",
    "        writer.add_scalar(\"train/beta\", beta_value, step)\n",
    "\n",
    "    if config[\"eval_interval\"] > 0 and step % config[\"eval_interval\"] == 0:\n",
    "        eval_rewards, eval_mean, eval_std = run_eval(\n",
    "            config[\"env_id\"],\n",
    "            n_episodes=config[\"eval_episodes\"],\n",
    "            seed_offset=10_000 + step,\n",
    "        )\n",
    "        eval_history.append({\"step\": step, \"mean\": eval_mean, \"std\": eval_std})\n",
    "        writer.add_scalar(\"eval/mean_return\", eval_mean, step)\n",
    "        writer.add_scalar(\"eval/std_return\", eval_std, step)\n",
    "        print(f\"Eval @ {step}: mean={eval_mean:.2f} +/- {eval_std:.2f}\")\n",
    "\n",
    "    if step % config[\"checkpoint_interval\"] == 0:\n",
    "        try:\n",
    "            state = agent.save_state()\n",
    "            ckpt_path = save_checkpoint(state, CKPT_DIR, step)\n",
    "            print(f\"\\u2713 Guardado checkpoint valido: {ckpt_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\u2717 Error guardando checkpoint: {e}\")\n",
    "        # Limpieza agresiva de memoria tras checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    if step % 500_000 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        sps = step / max(elapsed, 1e-6)\n",
    "        mean_100 = float(np.mean(episode_returns)) if len(episode_returns) > 0 else 0.0\n",
    "        eps_value = agent.eps_schedule.value(step)\n",
    "        print(\n",
    "            f\"Paso {step}/{config['total_steps']} | eps={eps_value:.3f} | \",\n",
    "            f\"return_ma100={mean_100:.2f} | {sps:.1f} steps/s\",\n",
    "        )\n",
    "\n",
    "    if done:\n",
    "        episode_returns.append(episode_reward)\n",
    "        train_episode_returns.append(episode_reward)\n",
    "        train_episode_end_steps.append(step)\n",
    "\n",
    "        # Solo loggear cada 10 episodios\n",
    "        if episode_count % 10 == 0:\n",
    "            writer.add_scalar(\"rollout/episode_return\", episode_reward, step)\n",
    "            writer.add_scalar(\"rollout/episode_length\", episode_len, step)\n",
    "            if len(episode_returns) > 0:\n",
    "                writer.add_scalar(\"rollout/episode_return_ma\", float(np.mean(episode_returns)), step)\n",
    "\n",
    "        episode_count += 1\n",
    "\n",
    "        # GC cada N episodios para liberar RAM\n",
    "        if episode_count % GC_EVERY_N_EPISODES == 0:\n",
    "            gc.collect()\n",
    "\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        episode_len = 0\n",
    "    else:\n",
    "        obs = next_obs\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Tiempo de entrenamiento (s): {train_time:.1f}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "save_final_artifacts(agent, FINAL_DIR, config, CONFIG_PATH)\n",
    "print(\"Modelo final guardado en:\", FINAL_DIR)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion formal\n",
    "Se evalua el agente final y una politica aleatoria con los mismos wrappers del entrenamiento.\n",
    "Se comparan 10 episodios de cada uno."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluacion formal final (10 episodios vs politica aleatoria)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "final_model_path = os.path.join(FINAL_DIR, \"final_model.pth\")\n",
    "\n",
    "if os.path.exists(final_model_path):\n",
    "    try:\n",
    "        agent.online.load_state_dict(torch.load(final_model_path, map_location=DEVICE))\n",
    "        agent.online.eval()\n",
    "        print(\"\\u2713 Cargado modelo final:\", final_model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"\\u2717 Error cargando modelo final: {e}. Buscando checkpoint...\")\n",
    "        latest_ckpt = get_latest_checkpoint(CKPT_DIR)\n",
    "        if latest_ckpt:\n",
    "            try:\n",
    "                state = load_checkpoint(latest_ckpt, device=DEVICE)\n",
    "                agent.load_state(state)\n",
    "                print(f\"\\u2713 Cargado checkpoint: {latest_ckpt}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"\\u2717 Error cargando checkpoint: {e2}. Evaluando agente en memoria.\")\n",
    "        else:\n",
    "            print(\"No se encontro checkpoint: se evalua el agente en memoria\")\n",
    "else:\n",
    "    print(\"Modelo final no encontrado. Buscando checkpoint...\")\n",
    "    latest_ckpt = get_latest_checkpoint(CKPT_DIR)\n",
    "    if latest_ckpt:\n",
    "        try:\n",
    "            state = load_checkpoint(latest_ckpt, device=DEVICE)\n",
    "            agent.load_state(state)\n",
    "            print(f\"\\u2713 Cargado checkpoint: {latest_ckpt}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\u2717 Error cargando checkpoint: {e}. Evaluando agente en memoria.\")\n",
    "    else:\n",
    "        print(\"\\u26a0 No se encontro modelo final ni checkpoints. Evaluando agente en memoria.\")\n",
    "\n",
    "FINAL_EVAL_EPISODES = 10\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUACION FORMAL (10 EPISODIOS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    eval_rewards, eval_mean, eval_std = run_eval(config[\"env_id\"], n_episodes=FINAL_EVAL_EPISODES)\n",
    "    print(f\"DQN+PER recompensa media: {eval_mean:.2f} +/- {eval_std:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\u2717 Error en evaluacion DQN+PER: {e}\")\n",
    "    eval_mean, eval_std = 0.0, 0.0\n",
    "\n",
    "try:\n",
    "    rand_rewards, rand_mean, rand_std = run_eval(\n",
    "        config[\"env_id\"],\n",
    "        n_episodes=FINAL_EVAL_EPISODES,\n",
    "        use_random=True,\n",
    "        seed_offset=999,\n",
    "    )\n",
    "    print(f\"Politica aleatoria recompensa media: {rand_mean:.2f} +/- {rand_std:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\u2717 Error en evaluacion aleatoria: {e}\")\n",
    "    rand_mean, rand_std = 0.0, 0.0\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Tabla comparativa y barplot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Policy\": \"DQN+PER\",\n",
    "            \"Mean\": eval_mean,\n",
    "            \"Std\": eval_std,\n",
    "            \"Episodes\": FINAL_EVAL_EPISODES,\n",
    "        },\n",
    "        {\n",
    "            \"Policy\": \"Random\",\n",
    "            \"Mean\": rand_mean,\n",
    "            \"Std\": rand_std,\n",
    "            \"Episodes\": FINAL_EVAL_EPISODES,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "display(results_df)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(results_df[\"Policy\"], results_df[\"Mean\"], yerr=results_df[\"Std\"], capsize=6)\n",
    "plt.title(\"BattleZone: Comparacion de rendimiento\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "Graficas de entrenamiento y evaluacion visibles en el notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Graficas de entrenamiento y evaluacion\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if \"train_episode_returns\" in globals() and len(train_episode_returns) > 0:\n",
    "    steps = np.array(train_episode_end_steps)\n",
    "    returns = np.array(train_episode_returns)\n",
    "    window = 100\n",
    "    if len(returns) >= window:\n",
    "        ma = np.convolve(returns, np.ones(window) / window, mode=\"valid\")\n",
    "        ma_steps = steps[window - 1 :]\n",
    "    else:\n",
    "        ma = None\n",
    "        ma_steps = None\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(steps, returns, label=\"episode_return\", alpha=0.5)\n",
    "    if ma is not None:\n",
    "        plt.plot(ma_steps, ma, label=f\"moving_avg_{window}\", linewidth=2)\n",
    "    plt.title(\"BattleZone - Train episode return\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay historial de entrenamiento en memoria para graficar.\")\n",
    "\n",
    "if \"eval_history\" in globals() and len(eval_history) > 0:\n",
    "    eval_steps = [e[\"step\"] for e in eval_history]\n",
    "    eval_means = [e[\"mean\"] for e in eval_history]\n",
    "    eval_stds = [e[\"std\"] for e in eval_history]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.errorbar(eval_steps, eval_means, yerr=eval_stds, fmt=\"-o\", capsize=4)\n",
    "    plt.title(\"BattleZone - Eval mean return vs step\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Mean Return\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay evaluaciones periodicas registradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video\n",
    "**Como grabar el video final (guion)**\n",
    "1) Mostrar 10-15s de entrenamiento: logs o TensorBoard con `episode_return` y `loss`.\n",
    "2) Mostrar 10-15s de ejecucion del agente en BattleZone.\n",
    "3) Cerrar con la tabla de evaluacion (mean/std) de 10 episodios."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Exportar un video corto de evaluacion\n",
    "final_model_path = os.path.join(FINAL_DIR, \"final_model.pth\")\n",
    "if os.path.exists(final_model_path):\n",
    "    agent.online.load_state_dict(torch.load(final_model_path, map_location=DEVICE))\n",
    "    agent.online.eval()\n",
    "_ = run_eval(config[\"env_id\"], n_episodes=1, seed_offset=9999, record_video=True)\n",
    "print(\"Video guardado en:\", VIDEO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reproducir video inline\n",
    "from IPython.display import Video, display\n",
    "import glob\n",
    "\n",
    "video_files = sorted(glob.glob(os.path.join(VIDEO_DIR, \"*.mp4\")))\n",
    "if not video_files:\n",
    "    print(\"No se encontraron videos en:\", VIDEO_DIR)\n",
    "else:\n",
    "    print(\"Mostrando:\", video_files[-1])\n",
    "    display(Video(video_files[-1], embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard (opcional)\n",
    "\n",
    "Ejecuta en Colab:\n",
    "\n",
    "```\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir artifacts/battlezone/logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporte tecnico\n",
    "\n",
    "**1) Justificacion (DQN+PER en BattleZone)**\n",
    "\n",
    "BattleZone es un entorno Atari de alta dimensionalidad visual con perspectiva en primera persona, un espacio de 18 acciones discretas y recompensas dispersas otorgadas por la destruccion de vehiculos enemigos (tanques, misiles, supertanques y platillos). Se selecciono DQN + Prioritized Experience Replay (PER) por las siguientes razones:\n",
    "\n",
    "- **DQN** es el baseline estandar para entornos Atari basados en pixeles, con arquitectura convolucional probada en los 57 juegos del Arcade Learning Environment (Mnih et al., 2015).\n",
    "- **PER** mejora la eficiencia muestral al priorizar transiciones con alto TD-error. En BattleZone, donde los encuentros con enemigos son esporadicos y la mayoria de frames contienen movimiento sin recompensa, PER asegura que las experiencias informativas (destruccion de enemigos, perdida de vida) se repitan con mayor frecuencia en el entrenamiento.\n",
    "- Con 18 acciones, la probabilidad de seleccionar aleatoriamente una accion util es ~5.5%, comparado con ~14% en Assault (7 acciones). PER compensa esta dificultad exploratoria al extraer mayor aprendizaje de cada experiencia relevante.\n",
    "\n",
    "**2) Algoritmo (resumen)**\n",
    "\n",
    "Se entrena una red Q convolucional (Nature DQN) sobre stacks de 4 frames en escala de grises de $84\\times84$ pixeles. El target de la red Q se calcula como:\n",
    "\n",
    "$$y = r + \\gamma \\max_a Q_{\\text{target}}(s', a) \\cdot (1 - \\text{done})$$\n",
    "\n",
    "La red objetivo se sincroniza con la red online cada `target_update_interval` pasos. PER usa un SumTree para muestreo proporcional con prioridades $p_i = |\\delta_i| + \\epsilon$ elevadas a $\\alpha$. Los pesos de importancia $(N \\cdot p_i)^{-\\beta}$ corrigen el sesgo de muestreo, con $\\beta$ que aumenta linealmente de 0.4 a 1.0 durante el entrenamiento.\n",
    "\n",
    "**3) Hiperparametros**\n",
    "\n",
    "| Parametro | Valor | Nota |\n",
    "|---|---|---|\n",
    "| env_id | ALE/BattleZone-v5 | 18 acciones, perspectiva 3D |\n",
    "| frame_skip | 4 | Estandar Atari |\n",
    "| clip_rewards | True | Normaliza recompensas a [-1, 1] |\n",
    "| total_steps | 3,000,000 | +50% vs Assault por mayor espacio de acciones |\n",
    "| learning_starts | 80,000 | Mas warmup para cubrir 18 acciones |\n",
    "| update_every | 4 | Frecuencia de updates |\n",
    "| batch_size | 32 | Estandar DQN |\n",
    "| buffer_size | 100,000 | Optimizado para RAM Colab (~3.5GB) |\n",
    "| gamma | 0.99 | Factor de descuento |\n",
    "| lr | 6.25e-5 | Mas conservador que Assault (1e-4); valor del paper Rainbow para entornos complejos |\n",
    "| target_update_interval | 10,000 | Sync target network |\n",
    "| checkpoint_interval | 500,000 | Para reanudar en multiples sesiones Colab |\n",
    "| eval_interval | 0 | Deshabilitado para ahorrar RAM |\n",
    "| epsilon | 1.0 -> 0.01 en 1.5M pasos | Decay mas lento (50% del total) |\n",
    "| PER alpha | 0.6 | Exponente de prioridad |\n",
    "| PER beta | 0.4 -> 1.0 en 2M pasos | Correccion de sesgo gradual |\n",
    "| seed | 123 | Reproducibilidad |\n",
    "\n",
    "**Justificacion de cambios vs Assault:**\n",
    "- **total_steps 3M** (vs 2M): 18 acciones requieren mas exploracion para descubrir combinaciones utiles.\n",
    "- **learning_starts 80K** (vs 50K): buffer necesita mas datos aleatorios diversos con 18 acciones.\n",
    "- **lr 6.25e-5** (vs 1e-4): LR mas bajo previene sobreescritura de Q-values para acciones poco visitadas. Valor recomendado en el paper Rainbow (Hessel et al., 2018).\n",
    "- **eps_decay_steps 1.5M** (vs 1M): mantiene ratio 50% decay/total, permitiendo exploracion prolongada.\n",
    "- **beta_steps 2M** (vs 1M): correccion de importancia mas gradual para estabilidad.\n",
    "\n",
    "**Optimizaciones para Colab (identicas a Assault):**\n",
    "- Buffer 100K (~3.5GB RAM vs 7.5GB con 200K)\n",
    "- Eval periodica deshabilitada (solo eval final)\n",
    "- Logging cada 1000 steps, episodios cada 10\n",
    "- GC forzado cada 50 episodios\n",
    "- Checkpoints cada 500K con validacion automatica\n",
    "\n",
    "**4) Librerias y versiones**\n",
    "\n",
    "Se imprimen en la celda de Setup:\n",
    "- Python 3.12.x\n",
    "- PyTorch 2.x + CUDA\n",
    "- Gymnasium 1.2.x\n",
    "- ale_py 0.11.x\n",
    "- TensorBoard\n",
    "\n",
    "**5) Hardware**\n",
    "\n",
    "Entrenado en Google Colab con GPU (A100 o T4 segun disponibilidad). Se imprime `nvidia-smi` en la celda de Setup.\n",
    "\n",
    "**6) Tiempo total de entrenamiento**\n",
    "\n",
    "Se imprime al finalizar el loop de entrenamiento. Estimado: 12-18 horas para 3M pasos en A100 (episodios mas largos que Assault). El sistema de checkpoints permite dividir en 2-3 sesiones de Colab.\n",
    "\n",
    "**7) Resultados cuantitativos (10 episodios)**\n",
    "\n",
    "Se reportan la media y desviacion estandar para DQN+PER y politica aleatoria en la tabla comparativa de la celda de evaluacion formal. En BattleZone, la recompensa se mide en puntos acumulados (sin clip) durante la evaluacion.\n",
    "\n",
    "Nota: dado que `clip_rewards=True` se usa tanto en entrenamiento como evaluacion, las recompensas reportadas estaran en escala clipeada (cada destruccion = +1.0 en vez de los puntos originales). Esto es consistente entre el agente entrenado y la baseline aleatoria.\n",
    "\n",
    "**8) Graficas**\n",
    "\n",
    "- **Train episode return**: retorno por episodio con media movil (ventana 100). Muestra la tendencia de aprendizaje.\n",
    "- Sin evaluaciones periodicas (para ahorrar RAM). TensorBoard contiene metricas adicionales (loss, TD-error, mean Q, epsilon).\n",
    "\n",
    "**9) Comportamiento aprendido y relacion con hiperparametros**\n",
    "\n",
    "BattleZone presenta desafios unicos comparado con otros entornos Atari:\n",
    "- **Perspectiva 3D**: los enemigos aparecen a distancias variables, requiriendo que la CNN aprenda invarianza de escala.\n",
    "- **Radar**: informacion adicional en la pantalla que indica posicion de enemigos fuera del campo de vision.\n",
    "- **18 acciones**: muchas combinaciones de movimiento + disparo. El agente debe aprender que acciones como FIRE y UPFIRE son utiles, mientras NOOP y movimientos laterales sin proposito no lo son.\n",
    "\n",
    "Comportamientos esperados del agente entrenado:\n",
    "- Avanzar hacia enemigos detectados (en vez de girar sin rumbo).\n",
    "- Disparar cuando hay un objetivo en el centro de la pantalla.\n",
    "- PER prioriza las experiencias de combate, permitiendo al agente aprender de encuentros enemigos esporadicos.\n",
    "- El decay lento de epsilon (1.5M pasos) asegura exploracion suficiente del espacio de 18 acciones.\n",
    "- El LR conservador (6.25e-5) previene que el agente \"olvide\" acciones utiles poco frecuentes.\n",
    "\n",
    "No se espera comportamiento optimo (humano ~37,800 puntos, Nature DQN ~26,300 con 200M frames). Con 3M pasos y buffer de 100K, el agente debe demostrar comportamiento no aleatorio: movimiento dirigido y disparo intencional.\n",
    "\n",
    "**10) Conclusiones y limitaciones**\n",
    "\n",
    "DQN+PER es una eleccion solida para BattleZone dado su espacio de acciones grande y recompensas dispersas. PER mitiga la ineficiencia muestral al priorizar experiencias de combate sobre frames vacios.\n",
    "\n",
    "**Limitaciones principales:**\n",
    "- Buffer de 100K es restrictivo para un entorno 3D complejo (la literatura recomienda 500K-1M).\n",
    "- 3M pasos es una fraccion del benchmark estandar (200M frames en los papers originales).\n",
    "- Sin evaluaciones periodicas, la visibilidad del progreso durante entrenamiento es limitada.\n",
    "- El reward clipping pierde informacion sobre el valor relativo de diferentes enemigos (tanque=1000, platillo=5000 -> ambos clippeados a 1.0).\n",
    "\n",
    "**Trade-offs realizados:**\n",
    "- Se sacrifico eficiencia muestral y diversidad de experiencia por estabilidad de ejecucion en Colab.\n",
    "- El LR mas bajo compensa parcialmente el buffer reducido al permitir actualizaciones mas estables.\n",
    "- El sistema de checkpoints habilita entrenamiento multi-sesion, critico para 3M pasos.\n",
    "\n",
    "**Comparacion con resultados de Assault:**\n",
    "- Assault (7 acciones): el agente aprendio a disparar efectivamente con 2M pasos.\n",
    "- BattleZone (18 acciones): se requieren mas pasos y exploracion mas prolongada, pero la arquitectura y el algoritmo escalan directamente al nuevo entorno."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
